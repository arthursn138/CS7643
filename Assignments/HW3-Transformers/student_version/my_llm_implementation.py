
###############################################################
### THIS FILE WAS AUTOGENERATED! BE CAREFULL WHEN EDITTING! ###͏︆͏󠄃͏󠄌͏󠄍͏︅͏︀͏︋͏︋͏︇͏︇͏︈
###############################################################
# file to edit: Transformer_Architectures.ipynb

###͏︆͏󠄃͏󠄌͏󠄍͏︅͏︀͏︋͏︋͏︇͏︇͏︈
    # Student: Arthur Scaquetti do Nascimento
    # GTID: 903721548
###

import os
os.environ["CUBLAS_WORKSPACE_CONFIG"]=":4096:2"
os.environ["CUBLAS_WORKSPACE_CONFIG"]=":16:8"
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset
from torch.utils.data.dataloader import DataLoader
from gtgpt.model import DummyMultiHeadedSelfAttention, DummyBlock, DummyTransformer, DummyEmbedding
from gtgpt.utils import set_seed
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
torch.set_default_device(DEVICE)

# #Do not change, it will break the AutoGrader
# setup_block = In[-1]

class Embedding(DummyEmbedding):
    def forward(self, idx):
        """
        :param idx: intTensor of shape (B,T)
        :returns embeddings: floatTensor of shape (B,T,n_embd)
        """
        B, T = idx.size()
        embeddings = None
        #############################################################################
        # TODO:
        # Implement the embedding lookup.                                           #
        #                                                                           #
        # This will take a few lines.                                               #
        #############################################################################

        # print(idx, idx.size())
        # # print(B)
        # # print(T)

        # # n_embeddings = 3    # From BERT paper

        ## From Piazza: idx = word indices; vocab_emb = embeddings for the words;
        # pos_emb = embeddings for relative positions

        emb_v = self.vocab_embeddings(idx)
        # emb_p = self.position_embeddings(idx[B])
        # print(emb_v.size())

        emb_p = torch.zeros_like(emb_v)
        # print('idx', idx.squeeze().tolist())
        # list_of_indices = idx.squeeze().tolist()
        # for i in list_of_indices:
        for i, word_index in enumerate(idx.squeeze()):
            # print(i, word_index)
            ii = torch.tensor(i)
            # print('ii', ii)
            emb_p[:, i, :] = self.position_embeddings(ii)
            # print('i=', i, '; emb_p=', emb_p[:, i, :])

        # print(emb_v)
        # print(emb_p)

        embeddings = emb_v + emb_p
        # print(embeddings)

        ##############################################################################
        #                               END OF YOUR CODE                             #
        ##############################################################################
        return embeddings


# #Do not change, it will break the AutoGrader
# embedding_def = In[-1]

class GenericSelfAttention(DummyMultiHeadedSelfAttention):
    def forward(self, x, attention_mask):
        """
        :param x: float Tensor of shape (batch size, sequence length, embedding dimensionality)
        :param attention_mask: int Tensor of shape (batch size, 1, sequence length, sequence_length)
        :returns y: float Tensor of shape (batch size, sequence length, embedding dimensionality)
        """
        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)
        y = None

        #############################################################################
        # TODO:                                                                     #
        # Implement multi-headed self-attention in GPT-2 Style                      #
        # Use the provided layers initialized in the DummySelfAttention constructor #
        # Apply dropout to the attention values after softmax and the final output  #
        #                                                                           #
        # Reference:                                                                #
        # https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention
        #                                                                           #
        # Note: All heads should be computed in parallel using the q,k,v layers     #
        #                                                                           #
        # For each item in the batch, if attention_mask[b, i, j] = 0,               #
        # then you should manually set the attention from token i to j to be -inf   #
        # Hint: See torch.masked_fill                                               #
        #############################################################################
        # print(x, x.size())

        ### From Piazza:
        ## Don't iterate over n_heads (single matmul should do the job)
        ## Should compute the layer norm **after each transformer layer** when iterating over them, rather than only after the final layer.

        k = self.k(x)
        q = self.q(x)
        v = self.v(x)

        # print('K:', k.size())   # D_X, D_Q
        # print('Q:', q.size())   # D_X, D_Q
        # print('V:', v.size())   # D_X, D_V
        # # print('Shape K', k.shape)

        ## Multihead part: "splitting" matrices into different heads (following https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention)
        # print('Number of heads =', self.n_head)
        # print(C / self.n_head)
        k = k.reshape(B, self.n_head, T, -1)   # B, H, Dx, Dq (slide notation)
        q = q.reshape(B, self.n_head, T, -1)   # B, H, Dx, Dq (slide notation)
        v = v.reshape(B, self.n_head, T, -1)   # B, H, Dx, Dv (slide notation)

        # ## Alternative way. This way passes next test, but can't have laryernorm to pass
        # k = k.view(B, T, self.n_head, int(C / self.n_head)).transpose(1, 2) # Ghazal's tip
        # q = q.view(B, T, self.n_head, int(C / self.n_head)).transpose(1, 2) # Ghazal's tip
        # v = v.view(B, T, self.n_head, int(C / self.n_head)).transpose(1, 2) # Ghazal's tip

        # # # print('Embedding dim:', C)
        # # # k.reshape(C, C, -1)

        # print('K:', k.size())   # H, Dx, Dq (slide notation)
        # print('Q:', q.size())   # H, Dx, Dq (slide notation)
        # print('V:', v.size())   # H, Dx, Dv (slide notation)

        similarities = torch.matmul(q, torch.transpose(k, 2, 3)) / torch.sqrt(torch.tensor(k.size(3))) # Normalized by Dq (slides notation)
        # print(similarities) # IF DOESN'T WORK, TRY TORCH.BMM<>TORCH.MATMUL<> @
        # mask1 = torch.where(attention_mask == 0, -float('inf'), attention_mask)
        # print('torch.where:', mask1)
        # mask = attention_mask.masked_fill(attention_mask == 0, float('-inf'))
        # # print('masked_fill:', mask2)
        masked_sim = similarities.masked_fill(attention_mask == 0, float('-inf'))
        # # # print('similarities', similarities)
        # # # print('masked_sim', masked_sim)
        # # # print('elementwise multiplication', similarities * mask)
        # # # # similarities = similarities * mask

        self.softmax = nn.Softmax(dim=3)
        softmax = self.softmax(masked_sim)
        # print(softmax)
        first_dropout = self.attn_dropout(softmax)
        # print(first_dropout)

        att = torch.matmul(first_dropout, v)
        # # # print('Before summing', att)
        # # # print(att.shape)
        # # # summed = torch.sum(att, dim=1)
        # # # print('Checking if summed over the correct dimension', summed)

        ## Concatenating heads' outputs and performing projections
        concat = att.reshape(B, T, C)   # Looks like it is reshaping the way I want

        # # # concat = torch.zeros_like(x)
        # # # for i in range(self.n_head):
        # # #     a = att[:, i, :, :]
        # # #     torch.cat((a, a))
        # # #     # concat[:, :, i] =

        # concat = att.contiguous().view(B, T, C) # This way passes next test, but can't have laryernorm
        # concat = att.transpose(1, 2).contiguous().view(B, T, C) # Matches Ghazal's tips. This way passes next test, but can't have laryernorm
        # # # print(concat)
        projected = self.c_proj(concat)
        # print(projected)

        y = self.hidden_dropout(projected)
        # print(second_dropout)
        # print(attention_mask)

        # self.normalization = nn.LayerNorm(C)
        # y = self.normalization(y)   #.detach().float()

        # print(y)
        # print(type(y))

        ##############################################################################
        #                               END OF YOUR CODE                             #
        ##############################################################################

        return y

# #Do not change, it will break the AutoGrader
# mha_def = In[-1]

#@title Now, we can very simply create a single layer transformer block!
class TransformerBlock(DummyBlock):
    def __init__(self, config):
        super().__init__(config, GenericSelfAttention)

    # A Basic Transformer Block with Attention followed by an MLP
    # note the layer norms and residual information preserved at each step.
    def forward(self, x, attention_mask):
        x = x + self.attn(self.ln_1(x), attention_mask)
        x = x + self.mlpf(self.ln_2(x))
        return x

# #Do not change, it will break the AutoGrader
# block_def = In[-1]

class GenericTransformer(DummyTransformer):
    def __init__(self, config):
        super().__init__(config, TransformerBlock, Embedding)
        self.block_size = config.block_size # Maximum Number of Tokens which can be encoded at once
        self.vocab_size = config.vocab_size

    def get_attention_mask(self, num_tokens):
        """
        Dummy For now, we will see how we use this later!
        """
        B = num_tokens.shape[0]
        return torch.ones((B, self.block_size, self.block_size))[:, :num_tokens.max().item(), :num_tokens.max().item()]

    def forward(self, idx, targets=None, hidden_cache=None, return_hidden=False):
        """
        :param idx: int Tensor of shape (B,T)
        :param hidden_cache: float Tensor of shape (B,P_T,n_embd)
        :param targets: int Tensor of shape (B,T_T)
        :param return_hidden: bool
        (if return_hidden = None)
        :returns x: float Tensor of shape (B,T,n_embd)
        (else)
        :returns logits: float Tensor of shape (B, T, vocab_size)
        :returns loss: float Tensor of shape (B) or None
        """
        num_tokens = (idx != -1).type(torch.int).sum(dim=1)
        if hidden_cache is not None:
          num_tokens = num_tokens + hidden_cache.shape[1]
        idx = idx.masked_fill(idx == -1, int(0)).type(torch.int)[:, :num_tokens.max().item()]
        if targets is not None:
          targets = targets[:, :num_tokens.max().item()]
        attention_mask = self.get_attention_mask(num_tokens)
        #############################################################################
        # TODO:                                                                     #
        # Put all the modules of a Transformer together for inference               #
        #                                                                           #
        # If hidden_cache exists,                                                   #
        # then the Transformer inputs should be concatenated in the token dimension #
        # First) All Embeddings from Hidden Cache                                   #
        # Next)  All Embeddings of tokens from idx.                                 #
        #                                                                           #
        # All the modules you'll need are listed here:                              #
        #                                                                           #
        # Note: You can iterate through a nn.ModuleList using a standard for loop.  #
        #                                                                           #
        # This will take a few lines!                                               #
        ##############################################################################

        ### From Piazza:
        ## Should compute the layer norm **after each transformer layer** when iterating over them, rather than only after the final layer.


        ##############################################################################
        #                               END OF YOUR CODE                             #
        ##############################################################################
        if return_hidden:
            return x

        # if we are given some desired targets also calculate the loss
        loss = None
        if targets is not None:
            s_logits = logits
            if hidden_cache is not None:
              s_logits = logits[:, hidden_cache.shape[1]-1:-1].contiguous()
              #print(logits[-1].argmax(dim=1))
            loss = F.cross_entropy(
                s_logits.reshape(-1, self.vocab_size), targets.reshape(-1), ignore_index=-1
            )


        return logits, loss

# #Do not change, it will break the AutoGrader
# transformer_def = In[-1]

class Encoder(GenericTransformer):
    """Encoder Style Transformer with Bidirectional Attention"""
    def get_attention_mask(self, num_tokens):
        """
        :param num_tokens: int Tensor of shape (batch size)
        :returns attention_mask: int tensor of shape (batch_size, 1, max_tokens, max_tokens)
        """
        B = num_tokens.shape[0]
        max_tokens = min(self.block_size, num_tokens.max().item())
        ##############################################################################
        # TODO:                                                                      #
        # Implement a padding mask function.                                         #
        # This allows batching sequences of different lengths.                       #
        #                                                                            #
        # For example, for any row attention_mask[b, i] the following should be true:#
        #               For j < num_tokens[b], attention_mask[b, i, j] = 1          #
        #               For j >= num_tokens[b],  attention_mask[b, i, j] = 0         #
        #                                                                            #
        # Reference:https://huggingface.co/docs/transformers/glossary#attention-mask #                                                                #
        #                                                                            #
        # This should be a 1-3 line function.                                        #
        ##############################################################################
        ##############################################################################
        #                               END OF YOUR CODE                             #
        ##############################################################################
        return attention_mask.reshape(B, 1, max_tokens, max_tokens)

# #Do not change, it will break the AutoGrader
# encoder_def = In[-1]

class Decoder(Encoder):
    """Decoder Style model with a Causal Attention Mask"""

    def get_attention_mask(self, num_tokens):
        """
        :param num_tokens: int Tensor of shape (batch size)
        :returns attention_mask: int tensor of shape (batch_size, 1, block_size, block_size)
        """
        full_attention_mask = super().get_attention_mask(num_tokens)
        ##############################################################################
        # TODO:                                                                      #
        # Modify the output of the full encoder mask to create a "causal" mask       #
        # such that tokens only attend to tokens which occured earlier in the input. #
        #                                                                            #
        # For example, for any row attention_mask[b, i} the following should be true:#
        #               For j <= i, attention_mask[b, i, j] = 1                      #
        #               For j > i,  attention_mask[b, i, j] = 0                      #
        #                                                                            #
        # This should be a one line function which modifies the full attention_mask  #
        ##############################################################################
        ##############################################################################
        #                               END OF YOUR CODE                             #
        ##############################################################################
        return attention_mask

# #Do not change, it will break the AutoGrader
# decoder_def = In[-1]

def generate(model, idx, max_new_tokens, temperature=1.0):
    """
    :param idx: int Tensor of shape (B, T)
    :param max_new_tokens: int
    :param temperature: Float
    :returns idx: int Tensor of shape (B, T+max_new_tokens)
    """
    ##############################################################################
    # TODO:                                                                      #
    # Sample from your model max_new_tokens times                                #
    # You should feed the predictions back into the model each time              #
    #                                                                            #
    # Adjust the probability distribution to be more or less greedy using        #
    # the temperature parameter                                                  #
    #                                                                            #
    # Reference: https://huggingface.co/blog/how-to-generate#sampling            #
    # Temperature Reference:                                                     #
    # https://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture10-nlg.pdf#page=34 #
    ##############################################################################
    ##############################################################################
    #                               END OF YOUR CODE                             #
    ##############################################################################
    return idx

# #Do not change, it will break the AutoGrader
# generate_def = In[-1]

class EncoderDecoder(nn.Module):
    """Encoder-Decoder Model which combines the two architectures"""
    def __init__(self, encoder_config, decoder_config):
        super().__init__()
        # Add end of sequence token.
        decoder_config.vocab_size += 1
        self.vocab_size = decoder_config.vocab_size
        self.encoder = Encoder(encoder_config)
        self.decoder = Decoder(decoder_config)

    def configure_optimizers(self, train_config):
        enc_groups = self.encoder.configure_optimizers(train_config)
        dec_groups = self.decoder.configure_optimizers(train_config)
        return enc_groups + dec_groups

    def forward(self, prefix, targets=None):
        """
        :param prefix: int Tensor of shape (B,P_T)
        :param idx: float Tensor of shape (B,P_T,n_embd)
        :returns logits: float Tensor of shape (B, vocab_size)
        :returns loss: float Tensor of shape (B) or None
        """
        B = prefix.shape[0]
        idx = torch.tensor([[]]).repeat(B, 1)
        if targets is not None:
          idx = torch.cat((idx, targets), dim=1)

        ##############################################################################
        # TODO:                                                                      #
        # Create an Encoder Decoder Model by combining your previous transformers    #
        # The Encoder should encode the tokens from prefix into an embeddings        #
        # Use these in the hidden_cache to condition decoder generation              #
        #                                                                            #
        # This should be a 1-2 lines.                                                #
        ##############################################################################
        ##############################################################################
        #                               END OF YOUR CODE                             #
        ##############################################################################
        return logits, loss

# #Do not change, it will break the AutoGrader
# encdec_def = In[-1]

def prefix_generate(model, prefix, max_new_tokens, temperature=1.0):
    """
    :param prefix: int Tensor of shape (B, T)
    :param max_new_tokens: int
    :param temperature: Float
    :returns idx: int Tensor of shape (B, max_new_tokens)
    """
    idx = torch.tensor([[]], dtype=torch.long)
    ##############################################################################
    # TODO:                                                                      #
    # Adjust your original generation function to work Encoder-Decoder models    #
    #                                                                            #
    # Note: This should be a one line change from the original generate function #
    ##############################################################################

    ### From Piazza: Pay close attention to the hint for prefix_generate function!
    ## The starting prefix and starting idx are provided in the boilerplate code.
    ## We mean "small change" quite literally - it should be at most 2 lines from the generate function if done correctly.
    ## (There is complementary info in Piazza via image)

    ##############################################################################
    #                               END OF YOUR CODE                             #
    ##############################################################################
    return idx

# #Do not change, it will break the AutoGrader
# pref_generate_def = In[-1]